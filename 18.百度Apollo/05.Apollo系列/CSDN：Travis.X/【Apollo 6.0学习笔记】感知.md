- [【Apollo 6.0学习笔记】感知_Travis.X的博客-CSDN博客](https://blog.csdn.net/Travis_X/article/details/121380690)

# 前言

**感知的目的是寻找机器的特长并发挥出来，向人类学习并做得比人类更好。**

机器感知系统需要根据各种各样的传感器来获取汽车周围的驾驶环境，包括Lidar、Camera、Radar，超声波雷达以及拾音器等。相对人类而言，机器感知是全覆盖，并且感知精度更高，能够达到厘米级别，但是机器感知在语义感知方面相差太大。

------

# 一、多维度剖析感知模块

感知模块可以从小感知和大感知两个维度进行划分

- 小感知：检测、分割、识别、跟踪、融合等 技术。
- 大感知：标定，定位，障碍物行为预测。

多维度看感知问题，体会复杂性。

- **传感器维度（输 入）** 。 主 要 有 Lidar ， Camera ， Radar ， Ultrasonic 高 精 地 图等。
- 维度（输出）。主要包括障碍物，车道线和道路边界，红绿灯等。
- **问题空间维度**。例如感知模块中涉及的一些算法或者要处理的问题，包括2D算法，3D算法（高度、距离、朝向角）。按照时间可以分为静态帧检测和时序处理等。
- **机器视觉维度**，分为高语义和低语义问题，例如模型计算和几何计算。
- **机器学习维度**，分为深度学习驱动的感知和通过先验知识进行启发式设计的方法，称之为后处理。
- **系统维度**，包括硬件和软件，以及软硬件一体化。在无人车系统里，感知的硬件显得更为重要，软件需要同步于硬件。

------

# 二、传感器选择和安装

目前，无人车中使用的传感器主要有激光雷达Lidar，相机和毫米波雷达Radar。

## 2.1 各传感器的特性

| 传感器      | 原理                       | 优点                                 | 缺点                                   |
| ----------- | -------------------------- | ------------------------------------ | -------------------------------------- |
| Lidar       | 主动式，ToF                | 测距准                               | 稀疏，感知范围近                       |
| Camera      | 被动式                     | 稠密感知，范围远                     | 测距不准                               |
| Radar       | 主动式，多普勒频移测速     | 测距，测速准                         | 噪点多，非金属障碍物召回低，无法做识别 |
| Ultrosonic  | 主动式                     | 近距离测距                           | 位置感知粗糙，只有近距离               |
| 高精地图    | 提前感知静态元素做先验     | 可以无差错精细感知，减轻线上感知负担 | 增加了高精地图和高精度定位依赖         |
| Image-Lidar | 接收器同时接收可见光和激光 | 原始就是4D数据                       | 感知范围近                             |

## 2.2 传感器的安装

![在这里插入图片描述](https://img-blog.csdnimg.cn/b9bdb852d7df45c8860197a6dddd0457.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVHJhdmlzLlg=,size_20,color_FFFFFF,t_70,g_se,x_16)
![在这里插入图片描述](https://img-blog.csdnimg.cn/2afd010c002549eaa1fb332a97efde56.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVHJhdmlzLlg=,size_20,color_FFFFFF,t_70,g_se,x_16)

------

# 三、传感器标定

在选好、安装完传感器之后，需要对传感器进行标定。标定的核心概念是得到传感器之间的相对位置，将不同传感器的数据在同一个坐标系中表示。标定分为**内参标定**和**外参标定**。内参是传感器自身性质，有些是厂家提供，有些需要自行标注，例如Camera焦距的定期矫正，Lidar中各激光管的垂直朝向角。外参是传感器之间的相对位置和朝向，一般由6个自由度表示，自由度的旋转矩阵和自由度的水平位移。

## 传感器标定案例

- Lidar内参标定：相对于摄像头，激光雷达的内参标定有一定的特殊性。
- Lidar-to-GPS外参标定：计算出Lidar与车上GPS的相对位置。GPS的位置由世界坐标系中的3D点表示，因此计算出外参之后可以计算得到Lidar在世界坐标系中的位置。
- Lidar-to-Lidar外参标定：计算不同Lidar之间的相对位置。
- Lidar-to-Camera外参标定：计算Lidar和相机之间的相对位置。
- Camera-to-Camera外参标定：计算不同相机之间的相对位置。
- 自然场景中的Lidar-to-Camera外参标定：在自然环境中，驾驶车辆进行两种不同传感器之间的位置关系求解。
- 自然场景中的Bifocal Camera外参标定：双焦点摄像头之间外参的计算，也是就是不同焦点的相对位置。
- Camera-to-Radar外参标定：摄像机到毫米波雷达的外参计算。

------

# 四、感知算法

## 4.1 点云感知

点云障碍物感知的主要任务是感知障碍物的位置、大小、类别、朝向、轨迹、速度等。核心是点云检测分割技术，可以用**启发式算法NCut**和**深度学习算法CNNSeg**完成。

### 4.1.1 启发式的Ncut

- 算法思路
  - 基于空间平滑性假设：空间上接近的点来自于同一障碍物
  - 基于点云构建Graph，G = (V, E , W )
  - 把点云检测问题建模成Graph分割问题，一个cluster是一个障碍物
  - 预处理（RO过滤，去地面）和后处理（异常过滤）
- 优缺点
  - 优点：解释性好；
  - 缺点： 分割规则过于简单，难以应对实际情况（草丛、绿化带），主要原因在于缺乏语义信息。

### 4.1.2 深度学习算法CNNSeg

主要思想是**利用卷积神经网络来处理激光雷达捕获的点云数据，并对点云中的目标进行识别**。深度学习是数据驱动的，它把人工构造特征这一任务交给机器和算法去完成（即特征学习）。

- 实现过程
  - 将所有点云都投到前视图（front-view）（投影面是一个圆柱面）来构造特征，将点云问题转化为矩阵问题，进而使用深度学习进行处理。通过构建全卷积神经网络对前向视图进行处理。
  - 借助自采集车队，采集更多的实际数据，并且扩展数据视角，制作俯视图，通过将俯视图+前视图相结合的方式进行训练。同时，修改Loss函数，包括使用3D回归和Segmentation的损失函数。
  - 因为俯视图没有高度信息，因此我们把前视图和Camara图像加进来进行辅助检查，利用Lidar的测距准和Camera识别准优势完成了Middle-Level Fusion方法（ Multi-View 3D ObjectDetection Network for Autonomous Driving），发表在CVPR 2017。该方法使用俯视图提取Proposal，利用前视图和光学图像辅助进行更加精准的位置回归。

------

## 4.2 视觉感知

视觉感知最早从ADAS发展而来。ADAS算法相对轻量，采用人工构造的特征和浅层分类器的方式实现辅助驾驶，该方法目前已经难满足自动驾驶的需求。随着深度学习技术的发展，尤其是在视觉领域的巨大成功，视觉感知的主流技术路线已经变为“**深度学习+后处理计算”**的方法。该方法带来了以下几个变化，第一是要求计算硬件升级，第二是数据的需求量大增，第三是如何评估保证安全性。

面向自动驾驶的深度学习算法具有以下几个特点：

- 2D感知向3D感知渗透，模型输出更丰富（后处理需要的3D信息、跟踪信息、属性信息等都会放在CNN中进行学习）
- 环视能力构建（传统方法靠一个Camera完成前向检测、碰撞检测、车道线检测。无人驾驶需要环视）
- 感知+定位+地图紧密结合

### 4.2.1 CNN检测

检测、2D到3D转换、跟踪三步是自动驾驶视觉感知的组成，后面两步都由CNN来学习，减少人工干预。

### 4.2.2 CNN分割

分割（Segmentation）与detection在本质上是一样的，是对一个目标的不同力度的刻画。分割是一种更细粒度刻画物体边界信息的检测方法，不再是画框，而是进行边缘分割。在感知中，对于不规则物体，往往需要进行分割处理。例如场景分割和可行驶区域感知。场景分割可以用于多传感器融合，例如对绿植进行分割，结合LiDAR点云，就可以测出绿植的类别和距离。可行驶区域则可以对一些非结构化道路的路径规划提供支持。在车道线感知中，应视具体情况使用分割或者检测方法。

### 4.2.3 后处理

一个完整的系统除了深度学习模型，还需要做一些后处理，后处理是直接针对下游模块，对后续的影
响比较直接。在视觉感知中，后处理主要分为三个部分。

- 第一是2D-to-3D的几何计算。2D到3D的转换需要考虑的因素包括：
  • 相机pose的影响
  • 接地点
  • 稳定性
- 第二是时序信息计算，主要是针对跟踪处理，需要注意以下几点：
  • 对相机帧率和延时有要求，要求跟踪必须是一个轻量级的模块，因为检测已经占据大部分时间
  • 充分利用检测模型的输出信息（特征、类别等）进行跟踪。
  • 可以考虑轻量级Metric Learning第三是多相机的环视融合
- 第三是多相机的环视融合
  •相机布局决定融合策略，要做好视野重叠 。

------

## 4.3 红绿灯感知

红绿灯感知的任务是在距离停止线50～-2米的范围内精准识别红绿灯亮灯状态。它的难点在于检测精度要求非常高，必须达到三个九（99.9%），否则会出现闯红灯，违法交规的情况。另外召回也不能太低，假设一直是绿灯，但是一秒只召回一帧绿灯，其他都认为是识别错的红灯，则通不过路口，影响通过率和体验。第二是红绿灯感知需要应对各种环境，包括天气和光照。最后是红绿灯的制式非常多，包括距离、高度、横纵、信号状态等，红绿灯感知都需要识别。

### 4.3.1 基于深度学习的红绿灯感知模块

自动驾驶中使用深度学习进行红绿灯感知模块的构建，主要分为以下几步。
1、相机选择和安装。在实际应用中使用了两个Camera，叫双Camera，长短焦切换，六毫米适用于近距离；
2、高精地图的交互。将部分固定信息交由高精地图去完成，减轻算法负担。
3、深度学习识别灯颜色的变化。主要分为检测和颜色分类两步。

------

## 4.4 Radar感知

Radar成的点云本身存在很大的噪声，需要对其进行鉴别，如下图可以用Radar反射信号来做高速路道路边缘栅栏的检测。
![在这里插入图片描述](https://img-blog.csdnimg.cn/48a696bb13ce418193fd78db281305ce.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVHJhdmlzLlg=,size_20,color_FFFFFF,t_70,g_se,x_16#pic_center)

------

## 4.5 超声波感知

超声波只能进行近距离感知，并且无法感知具体的位置。对无人驾驶来说，行驶过程中帮助并不大，更多的是用于倒车和特别近距离的感知。

------

# 参考

【1】《[Apollo](https://so.csdn.net/so/search?q=Apollo&spm=1001.2101.3001.7020)进阶课程16 | 感知概貌》
【2】《Apollo进阶课程17 | 传感器选择与安装》
【3】《Apollo进阶课程18 | 传感器标定》
【3】《Apollo进阶课程19 | 感知算法》
【3】《Apollo进阶课程20 | 机器学习与感知的未来》