- [自动驾驶p_zashizhi3299的博客-CSDN博客](https://blog.csdn.net/zashizhi3299/article/details/109702724)

1、纯视觉感知；

2、激光雷达、高精地图、相机的融合感知。

# 完成自动驾驶的三个前提

1、环境感知。行车环境车道线、路牙、道路隔离物、恶劣路况，周边物体车辆，行人以及交通标志的识别需要雷达、摄像机等传感器；

2、车辆自我状态感知（定位与车速）。GPS与IMU；

3、车辆操控。信息融合。

# 深度学习自动驾驶的2种结构

1、串行“感知-规划-执行”流水线。 使用AI和深度学习方法或基于经典的非机器学习方法设计组件。

2、端到端系统，end-to-end。主要基于深度学习方法。

# Udacity无人驾驶课程内容

- 计算机视觉：道路检测，高级道路检测。两个用纯计算机视觉的方法来识别车道线，一个是直线检测，另一个是利用像素点。
- 机器学习：交通标志识别，行为克隆，车辆识别。前两个都是运用目前很火的深度学习，第一个就是常见的图片分类，第二个是在模拟器里面训练一个端对端（相机视觉->驾驶信号）的神经网路来控制汽车，第三个是则是用的SVM。
- 传感器融合：卡尔曼滤波，扩展卡尔曼滤波。扩展卡尔曼滤波主要为了解决非线性的问题。
- 定位：蒙特卡洛滤波（粒子滤波）。一种马可夫定位方式。
- 轨迹规划：高速驾驶。通过行为规划，轨迹规划来为在高速公路上驾驶的汽车规划路线。
- 控制：PID控制，MPC。PID是工业上最广泛应用的控制方式，MPC则是比较新的控制模型。

# 特斯拉

Andrej说道：“很多自动驾驶车辆在车顶装上昂贵的激光雷达，就能够感知360度的环境，并测量距离。不过，如果要使用激光雷达，还需要高精地图的匹配。根据高精地图存储的车道线、红绿灯等信息，就能够实现自动驾驶。”
不过，特斯拉所走的路线完全不同。特斯拉希望基于环绕车身的8个摄像头实现自动驾驶。当车辆行驶在道路上时，需要自动驾驶电脑判断哪里是车道线、哪里是红绿灯。
也就是说，**激光雷达+高精地图的技术路线本质上是匹配的过程，而纯视觉技术路线则是寻找的过程。**
同时，Andrej指出，纯视觉方案显然是一个更具拓展性的技术路线。当前，特斯拉在全球范围内已经交付了百万辆汽车，因此打造一套全球范围的高精地图并不现实，因此特斯拉选择了纯视觉的技术路线。
他说道：“**想要实现纯视觉感知实际非常困难，需要基于视频打造非常优秀的神经网络（纯视觉感知难点）**。不过，一旦纯视觉系统真正投入使用，将是一套通用的系统，并且理论上全球任何区域都能使用。”决定采用纯视觉感知的特斯拉有三个需求，第一是大量的视频数据，百万段规模的视频；第二是干净的数据，包括对物体标签化，并且拥有深度、速度、加速度信息；第三是多样化的数据，大量的边缘案例。此后，特斯拉需要在此基础上训练一个足够大的神经网络。
几年前特斯拉开始研发Autopilot时，就已经确定不使用高精地图和激光雷达，全车的传感器有8枚摄像头和1个毫米波雷达。从几年的成果来看，无论是实际效果还是功能丰富程度，特斯拉在量产自动驾驶行列中一直处于头部。

***\*三个典型案例\****说明纯视觉传感器的表现再次胜过毫米波雷达+视觉传感器的融合模式
第一个案例是前方车辆快速刹车时，毫米波雷达在其中有多次出现距离突然降低为0、速度突然提升、加速度突然为0的情况。这是因为突然减速之后，毫米波雷达并不能很好追踪前方车辆，因此多次重启，就像是车辆在短时间内重复消失，又出现了6次，这很可能误导自动驾驶系统。
由纯视觉传感器感知的信息和毫米波雷达的信息大致重合，但是没有出现距离、速度、加速度突变的情况，表现非常稳定。
第二个案例是路上常见的立交桥。由于毫米波雷达没有垂直分辨率，所以会认为立交桥是一个静止在前方的物体。传感器感知数据融合之后，车辆认为行驶前方存在静止物体，并判断紧急刹车。这种情况在高速公路上十分危险。
第三个案例是行驶前方出现一辆停靠路边的大货车。此时，毫米波雷达无法判断前方究竟是一辆车还是一个普通的静止物体。因此，毫米波雷达将这项任务交给视觉传感器，最终在距离车辆110米的距离时，才感知到前方的卡车。如果用纯视觉传感器，在180米的距离时感知到了前方的货车，并从145米处有明确的感知信息，开始减速。

# 本田

本田获得日本交通省的批准，2021年3月将在本田[Legend](https://so.csdn.net/so/search?q=Legend&spm=1001.2101.3001.7020)车型上实现搭载L3级自动驾驶技术(Traffic Jam Pilot)。在日本《道路车辆法》修正案允许L3级自动驾驶汽车上路

核心还是对于车道维持、变道和堵车时候的时机切换。

本田这里使用了5个激光雷达和摄像头在一个独立的控制器里面进行感知融合，然后采用5个毫米波雷达和摄像头在另外一个控制器里面做融合，交给一个高算力主控单元，把车辆位置（通过高[精度](https://so.csdn.net/so/search?q=精度&spm=1001.2101.3001.7020)地图定位）、驾驶员状态感知、声光和触感的反馈还有执行部件（制动和转向的冗余，额外的DCDC+12V电池供电系统）。 

L3的主要挑战是把车辆驾驶从自动驾驶系统移交给驾驶员，本田设计的方式是通过指示器显示的视觉（光），通过警报的听觉（声）以及通过安全带振动的触感（体感），这三种机制下如果驾驶仍然不响应，系统将会在路肩上紧急停车。在这个过程里，是配置监控摄像头，可不断监控驾驶员的状况。

# 美国汽车工程师学会（SAE）的定义

**L0：**无自动化。

**L1：**原始驾驶员辅助系统（Primitive driver assistance systems），包括自适应巡航控制、防抱死制动等。

**L2：**部分自动化，先进的辅助系统（Advanced assistance systems），例如紧急制动或避免碰撞。

**L3：**有条件的全自动化（Conditional automation），在正常操作期间，驾驶员可以专注于除驾驶以外的其他任务，但是紧急情况下必须能快速响应并接管车辆。

**L4：**在天气条件许可，基础设施（信号地图等）完善的情况下，完全不需要驾驶员。

**L5：**无论在任何场景下，都不需要驾驶员，目前尚无完全实现L4级别及以上的自动驾驶车辆。

ADS（Automatic driving system）自动驾驶系统

ADAS（advanced driving assistance system）高级驾驶辅助系统

# 谷歌Waymo

就是

![img](https://img-blog.csdnimg.cn/20211003121120828.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

# 百度阿波罗Apollo

就是

![img](https://img-blog.csdnimg.cn/20211003121151523.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

# 通用Curise

就是

![img](https://img-blog.csdnimg.cn/20211003121304244.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

# **传感器**

不同传感器感知距离：波长、频率、波速间的关系： v= λ T 或 v=λf。（*光速v*为30万公里/秒）。

![img](https://img-blog.csdnimg.cn/img_convert/627395c4c57b3e2ab0eb9665e2a9100a.png)

传感器安装位置：

![img](https://img-blog.csdnimg.cn/img_convert/c99817d76d7e82507b78bdc3b62c8391.png)

不同传感器在自动驾驶中的功能：

![img](https://img-blog.csdnimg.cn/20211003113049146.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

### 1、相机

单目相机（Monocular Cameras）是最常见最廉价的传感器之一，且2D的计算机视觉是较成熟的研究领域，虽然理论上无法获得深度，但是现在也有一些基于单目深度的结果，缺点主要还是在精度和容易受环境因素影响上。

### 2、雷达Radar（radio detection and ranging）

波长、频率、波速的关系： v= λ T 或 v=λf。（波速为光速=30万公里/秒）。

**灰尘烟雾的颗粒大小处于激光波长和毫米波波长之间，所以在面对灰尘烟雾的时候毫米波有较好的穿透效果，而激光雷达无法穿透，会将其识别为障碍物。**

![img](https://img-blog.csdnimg.cn/20200418112133751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3psYl96bGI=,size_16,color_FFFFFF,t_70)



### 根据雷达波长频率分类：

**毫米波雷达**，是工作在毫米波波段**MMW**（millimeter wave）探测的雷达。通常30～300GHz频域，波长1～10mm。属于**电磁波雷达**，意思为"无线电探测和测距"。雷达发射电磁波对目标进行照射并接收其回波，由此获得目标至电磁波发射点的距离、距离变化率（径向速度）、方位、高度等信息。

![img](https://img-blog.csdnimg.cn/20211003120737867.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_10,color_FFFFFF,t_70,g_se,x_16)

毫米波雷达原理上使用**“多普乐”效应**，多普勒频移的连续波（CW）雷达。**优**势是相对速度估计的非常准确，对雨雾不敏感。**缺**点是分辨率仍然赶不上相机和激光雷达，对于金属敏感即是优点也是缺点。多普勒效应毫米波雷达原理：当波源和观测者相互接近时，接受到的频率升高；两者相互离开时降低。脉冲多普勒雷达，当雷达持续发射一固定频率的脉冲波对空扫描时，如遇到活动目标，回波的频率与发射波的频率的频率差，称为多普勒频率。根据多普勒频率的大小，测出目标对雷达的径向相对运动速度；根据发射脉冲和接收的时间差，测出目标的距离。

![img](https://img-blog.csdnimg.cn/e6b68801276c47499f0834671465ab71.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

ARS408-21毫米波雷达传感器在一个测量周期内独立测量物体的距离和速度（多普勒原理），**FMCW**（Frequency Modulated Continuous Wave，频率调制连续波）技术。具有非常快的斜坡基础，具有每秒17次的实时扫描功能。该设备的特点是能够同时测量250m距离、相对速度和两个对象的角度关系（测速、测距、方位角估计）。

**激光雷达Laser Radar**。目前市场上三维成像激光雷达最常用的波长是905nm和1550nm。由于905nm的波长接近人眼感知范围（可见光的波长400～780nm），所以不能以太高功率运行，否则容易伤眼；1550nm波长**LiDAR**（Light Detection And Ranging）可以以更高的功率运行，以提高探测范围，同时对于雨雾的穿透力更强（波长越长穿透力越强），LiDAR是一种集激光、全球定位系统和惯性导航三种技术与一身的系统，用于获得数据并生成精确的DEM。905nm的主要优点是硅在该波长处吸收光子，而硅基光电探测器通常比探测1550nm光所需的铟镓砷(InGaAs)近红外探测器便宜。发射激光束探测目标的位置、速度等特征量的雷达系统。

![img](https://img-blog.csdnimg.cn/20211003120726228.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_10,color_FFFFFF,t_70,g_se,x_16)

其工作原理是**光沿直线传播**，向目标发射探测信号（**激光**束），然后将接收到的从目标反射回来的信号与发射信号进行比较，当处理后可获得目标的有关信息：如目标距离、方位、高度、速度、姿态、甚至形状等参数。激光器将电脉冲变成光脉冲发射出去，光接收机再把从目标反射回来的光脉冲还原成电脉冲，送到显示器。激光雪达的**优**勢是测距准确分率较高，对物理尺寸有一个稳定的检测。**缺**点激光雷达寿命无法赶上整车质保的要求，且成本高。另外受雨雪、纸片等干扰感知能力。根据测距方法的不同可以分为三角法测距、**TOF法**测距、相干法测距。激光雷达根据线束的多少，分为16线，32线，64线，以及128线激光雷达。 

激光雷达传感器的基本元件是发射器、接收器（APD，雪崩光电二极管）和处理单元。 激光雷达传感器通过将光速乘以从发射器到接收器APD的脉冲行进时间的一半来测量距离。激光雷达处理单元对接收到的脉冲应用滤波器和取阈值，以区分噪声检测和有意义的检测。 同时，接收的激光雷达脉冲携带关于材料和检测到的目标反射性的信息。这些信息的表示，要么是检测到的反射强度，要么是检测到的反射**回波脉冲宽度（EPW）**，取决于激光雷达处理单元在其字节流计算或返回什么。

**超声波雷达**是量产最多的雷达系统（Ultrasonic sensor），常用探头的工作频率有 40kHz、48kHz，检测距离较短1.5m，量产车几乎是标配，用于倒车和泊车。

《**Sensing system of environmental perception technologies for driverless vehicle_ A review of state of the art and challenges**》

### 按激光雷达作用分类：

激光测距雷达。

激光测速雷达。

激光成像雷达。一种基于3D激光雷达的障碍物自适应网格表示方法。该方法利用障碍物密度与光栅图像分辨率之间的函数关系，快速表达障碍物。

跟踪雷达。使用多个激光传感器检测无人驾驶车辆动态障碍物的方法。检测动态障碍物的位置，消除传感器数据处理延迟引起的位置偏差。

### 按激光雷达线数分类

单线。是一种具有一维旋转扫描的高频脉冲激光测距仪

多线。是激光雷达的多个发射源发射的激光。最大优点是它可以跟踪多个对象的轨迹。

### 按激光雷达发射波形分类

脉冲型。输出是不连续的，可以达到几飞秒的量级，因此通常用于测量超快物理过程。

连续型。使用连续激光输出。在频率稳定后可以获得非常窄的线宽，可用于激光测距。

###  3、全球定位系统(Global Positioning System，GPS)

`全球定位系统`输出常见为：`经度，维度，和高度。第`一个载波驻留在L1波段（中心频率为1575.42MHz），第二个驻留在L2波段（中心频率为1227.6MHz）。L1波段主要是民用，包含了两种代码，一个叫做粗捕获码（C/A）码，另一个叫做精测距码（P码）。L2波段只用于军用场合，仅含有一个P码。所有24个卫星的L1信号均使用同样的频率，但相互不发生干扰，因为它们每一个都经由覆盖了2.046MHz波段的一个PRN代码进行了扩频。经过PRN代码扩频后的GPS信号不仅能区别于其他信号，还具有抗干扰能力。

GNSS（global navigation satellite system）全球导航卫星系统，它包括了GPS，欧盟的伽利略（Galileo）；俄罗斯的格洛纳斯卫星导航系统（GLONASS）；中国的北斗（BDS）等。

### 4、惯性测量单元IMU`（Inertial Measurement Unit）`

加速度传感器。 **加速度 = 作用力/惯性(质量)。**IMU要安装在被测物体的重心上，测量物体三轴姿态角（或角速率）以及加速度的装置。一般IMU包含了3个单轴的加速度计和3个单轴的陀螺（测量角速度）。

![img](https://img-blog.csdnimg.cn/20201122211224765.png?#pic_center)

IMU更新频率高可达1000赫兹，所以IMU可以提供接近实时的位置信息。但缺点在于其运动误差随时间增加而增加，只能在很短的时间范围内进行定位。但是结合GPS来定位汽车，一方面IMU弥补了GPS更新频率较低的缺陷。另一方面GPS纠正了IMU的运动误差。

# 目标检测

算法性能比较。

![img](https://img-blog.csdnimg.cn/img_convert/e793eb6611ef7d3a1290ba06090e2639.png)

# 语义分割

算法性能比较。

![img](https://img-blog.csdnimg.cn/img_convert/5866f645625cf7a853ce5c0b43399b82.png)

# **定位**

定位子系统负责估计自动驾驶汽车相对于地图或道路的姿态(位置和方向)。大多数通用定位子系统都是基于GPS的。但总的来说，它们并不适用于城市自动驾驶汽车，因为在树木下、城市峡谷(被大型建筑包围的道路)或隧道等闭塞区域，GPS信号无法得到保证。

不依赖GPS的定位方法。主要分为三类：基于激光雷达的、基于激光雷达加相机的和基于相机的。基于激光雷达的定位方法提供了测量精度和易于处理。然而雷达价格很高。激光雷达加摄像头定位方法中，使用激光雷达数据构建地图，利用摄像头数据估计无人驾驶汽车相对于地图的位置，降低了成本。基于摄像头的定位方法既便宜又方便，但通常不太精确和/或可靠。

![img](https://img-blog.csdnimg.cn/20211003121905170.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

### 1、基于Lidar激光雷达的定位

经典方法提出了一种结合三维点配准算法的多层自适应蒙特卡罗定位（ML-AMCL）方法。为了估计汽车姿态，从三维激光雷达测量中提取水平层，并使用单独的AMCL实例将层与使用三维点注册算法构建的三维点云地图的二维投影对齐。对于每个姿态估计，对一系列的里程测量进行一致性检查。将一致的姿态估计融合到最终的姿态估计中。该方法在实际数据上进行了评估，得到相对于GPS参考的位置估计误差为0.25m。然而，三维地图存储量太大。Veronese等人提出了一种基于MCL算法的定位方法，该方法通过二维在线占有栅格地图和二维离线占有栅格地图之间的地图匹配来校正粒子的姿态。评估了两种地图匹配距离函数：改进了传统的两个栅格地图之间的似然场距离，以及两个高维向量之间的自适应标准余弦距离。对IARA自动驾驶汽车的实验评价表明，利用余弦距离函数，定位方法可以在100hz左右工作，横向和纵向误差分别为0.13m和0.26m。

### 2、激光雷达和相机方式定位

一些方法利用激光雷达数据建立地图，利用摄像机数据估计自动驾驶汽车相对于地图的位置。Xu等人提出了一种立体图像与三维点云地图匹配的定位方法。地图由一家地图公司（[http://www.whatmms.com](https://link.zhihu.com/?target=http%3A//www.whatmms.com)）生成，由几何数据（纬度、经度和海拔）和从里程表、RTK-GPS和2D激光雷达扫描仪获取的缓解数据组成。他们将地图的三维点从真实坐标系转换到摄像机坐标系，并从中提取深度和强度图像。采用MCL算法，通过将汽车摄像机拍摄的立体深度和强度图像与从3D点云地图中提取的深度和强度图像进行匹配来估计汽车的位置。该方法在实际数据上进行了评估，并给出了0.08 m到0.25 m之间的位置估计误差。VIS16提出了一种将地面全景图与一年中不同季节拍摄的卫星图像相匹配的自动驾驶汽车定位方法。在他们的方法中，激光雷达数据被分为地面/非地面类别。接下来，利用激光雷达数据将全景相机拍摄的自驾车地面图像分割成地面/非地面区域，然后进行扭曲以获得鸟瞰图。利用kmeans聚类将卫星图像分割成地面/非地面区域。然后利用MCL将鸟眼图像与卫星图像进行匹配，估计姿态。该方法在NavLab11自动驾驶汽车上进行了验证，获得了3m~4.8m的位置估计误差。

### 3、基于相机的定位方式

有些方法主要依靠摄像机数据来定位自驾车。Brubaker等人提出了一种基于视觉里程和道路地图的定位方法。他们使用OpenStreetMap，从中提取出感兴趣区域内连接他们的所有十字路口和所有可行驶道路（以分段线性段表示）。然后建立了一个基于图的路线图表示法和一个汽车如何通过该图的概率模型。利用这个概率模型和视觉里程测量，他们估计汽车相对于路线图的位移。使用递归贝叶斯滤波算法，通过利用图形的结构和车辆如何移动的模型（通过视觉里程计测量）在图形中执行推断。该算法能够通过增加当前姿势位于与最新汽车运动（直线行驶距离和最近的曲线）相关的图形点的概率，并通过降低其位于不相关的点的概率来精确定位汽车在图形中的位置。定位方法在KITTI视觉里程数据集上进行评估，并在行驶52秒后能够在包含2150公里可行驶道路的18平方公里地图上定位车辆，精度为4米。一些方法使用相机数据来构建特征地图。Ziegler等人描述了自主车辆Bertha在历史悠久的Bertha-Benz纪念路线上自主驾驶所使用的定位方法。提出了两种基于互补视觉的定位技术：基于点特征的定位（PFL）和基于车道特征的定位（LFL）。在PFL中，使用从当前相机图像中提取的DIRD描述符，将当前相机图像与先前在映射过程中获取的相机图像序列的图像进行比较。从映射过程中捕获的图像的全局位置恢复全局位置估计。在LFL中，地图半自动计算，提供道路标记特征（水平道路信号）的全局几何表示。通过检测从摄像机图像的鸟瞰图中提取的道路标记特征并将其与存储在地图中的水平道路信号相关联，将当前摄像机图像与地图匹配。然后，由PFL和LFL获得的位置估计被Kalman滤波器组合（作者不提供组合定位误差的估计）。Jo等人提出了类似于LFL的定位方法。一些方法使用相机数据来构造特征地图，但采用了其他类型的特征。Radwan等人提出了一种基于文本特征检测的定位方法。现成的文本提取技术用于识别环境中的文本标签。采用MCL算法对多个观测值进行融合。该方法在实际数据上进行了评估，并给出了1 m到25 m之间的位置估计误差。Spangenberg等人提出使用杆状地标作为主要特征，因为它们是独特的、长期稳定的，并且可以被立体摄像机检测到。此外，它们允许内存高效的映射表示。特征检测主要由立体摄像机完成。定位采用MCL算法结合Kalman滤波器进行鲁棒性和传感器融合。该方法在自主车辆上进行了评估，得到了0.14m到0.19m之间的位置估计误差。一些方法建议使用神经网络对自动驾驶汽车进行定位。它们由相关的摄像机图像和相关的全局位置组成。在映射阶段，神经网络建立环境的表示。为此，它学习一系列图像和图像被捕获的全局位置，这些位置存储在一个神经地图中。在定位阶段，神经网络利用神经网络地图提供的先验知识来估计当前观测图像的全局位置。这些方法存在仪表刻度误差，难以实现大面积自主车辆的定位。

### **4、GPS-IMU融合**

GPS-IMU融合的主要原理是用绝对位置数据修正航位推算（dead reckoning）的累积误差。在GPS-IMU系统中，IMU测量机器人位置和方向的变化，并对这些信息进行处理，以便用航位推算法对机器人进行定位。但是IMU有一个显著的缺点，就是我们常说的累积误差。因此引入GPS的绝对位置信息（相当于一个反馈），可以有效地对IMU误差进行校正。GPS-IMU融合的方法的精度比较低，实际上并不能直接用在车辆定位上。在密集的城市环境中，像隧道，高层建筑等都会影响GPS的精度。尽管GPS-IMU系统本身无法满足自动驾驶的性能要求，但是可以和激光雷达等传感器相结合进行位姿估计。

### **5、基于先验地图定位**

基于先验地图的定位技术的核心思想是**匹配**：定位是通过比较在线数据同先验地图的信息来找到最佳匹配位置。也就是根据先验的地图信息来确定当前的位姿。这个方法有一个缺陷，一般需要额外的一个地图制作步骤，而且，环境的变化可能会对结果产生负面影响（比如光照变化，参照物移动等）。

这类方法大致可以分为两大类：**基于路标的定位和基于点云的匹配**。- **基于路标**：与点云匹配相比，基于路标的定位计算成本要低得多。理论上来说，只要路标的数量足够多，这种定位就是鲁棒的。[15]中采用了激光雷达和蒙特卡罗结合的方法，通过匹配路标和路缘（road markers and curbs）来定位车辆的位置。[16]介绍了一种基于视觉的道路标记（road marking）检测方法，事先保存了一份低容量的全局数字标记地图，然后与前置相机的采集数据进行比较。最后根据检测结果和GPS-IMU输出利用粒子滤波器进行位置和方向的更新。该方法的主要缺点在于地标的依赖性**。基于点云**：点云匹配一般是指局部的在线扫描点云通过平移和旋转同先验的全局点云进行匹配，根据最佳匹配的位置来推测机器人相对地图的局部位置。对于初始位姿的估计，一般是结合GPS利用航位推算。

# **环境地图建图**

离线和在线建图子系统负责计算自动驾驶汽车运行环境的地图。这些子系统是让它们在非结构化环境中导航而不与静态障碍(如路标、障碍物等)发生冲突的基础。

环境的表示通常区分为拓扑和度量。**拓扑**表示将环境建模为一个图，其中节点表示重要位置(或特征)，边表示它们之间的拓扑关系(例如，位置、方向、邻近和连通性)。这些分解的分辨率取决于环境的结构。**度量**表示通常将环境分解成有规则间隔的单元。这种分解并不依赖于特征的位置和形状。度量表示的空间分辨率往往高于逻辑表示的空间分辨率。这使它们成为最常见的空间表示。度量表示的最重要的计算方法，它可以进一步细分为规则间距网格表示和可变间距网格表示。

**规则间距网格度量地图**有：

它将环境离散为一个固定大小的单元矩阵，该矩阵包含有关是否属于某条道路的信息以及移动到其相邻单元的成本。道路网格地图简单易懂。然而，如果移动成本在路线图的大范围内是一致的，那么使用网格表示可能需要浪费内存空间和处理时间。 路线点序列是压缩大型道路网格地图中路径描述的一种替代方法。路线点是沿路线栅格地图中的路径的点。路线点序列可以手动定义，也可以自动从道路网格地图中提取。

Moravec & Elfes(1985)提出的**占用网格地图(OGM)**。OGM将空间离散成固定大小的单元，通常以厘米为数量级。每个cell都包含与之相关的区域被占领的概率。利用传感器数据独立更新每个小区的占用概率。为了简单和高效的目的，可以将描述环境特征的3D传感器测量投影到2D地平面上。假设每个单元的占用概率是独立的，使得OGM算法的实现又快又容易。然而，它生成了**稀疏的空间表示**，因为只有传感器接触的细胞被更新。

Thrun和Montemerlo提出了GraphSLAM算法。GraphSLAM是一种离线同步定位和映射(SLAM)算法，它从传感器数据中提取一组软约束，用稀疏图表示。通过将这些约束条件分解为一个全局一致的机器人姿态估计，它能够获得环境地图和传感器数据捕获过程中所遵循的机器人路径。这些约束与传感器捕获的数据的属性有关(例如，两个连续的里程测量之间的距离和运行时间)，通常是非线性的，但在解决它们的过程中，它们被线性化，并使用标准的优化技术来解决最小二乘问题。一旦估计出机器人的姿态，就可以使用OGM计算的概率算法从传感器数据构建地图。

**可变间距网格度量地图**有：

八叉树图OctoMaps，它以不同的3D分辨率存储信息。与具有不同3D分辨率的OGM相比，OctoMaps只存储观测到的空间，因此内存效率更高。然而，它们是计算密集型，目前可用的硬件不适合自动驾驶汽车。此外，典型的自动驾驶汽车可以建模为一个平行六面体，或一系列相互连接的平行六面体，地图可能只包含与环境中阻碍这些平行六面体运动的物体有关的信息。

Droeschel提出的混合地图，它存储了不同分辨率的占用率和距离测量值。为此，测量数据存储在网格单元中，网格单元的大小从汽车的中心开始递增。因此，在传感器附近具有较高的分辨率，可以提高计算效率。这遵循一些传感器关于距离和测量密度的特性(例如，激光传感器的角度分辨率)。

Doherty提出的高斯过程占用图(GPOM)。GPOM使用高斯过程(GP)从训练数据集中的稀疏传感器测量数据中学习环境的结构，然后估计未被传感器直接拦截的细胞占用的概率。实验表明，定位误差比粒子滤波定位和OGM估计的低三倍以上。然而，该推理具有较高的计算成本。

Ramos提出的希尔伯特图。希尔伯特映射用线性判别模型表示占用概率，该模型在高维特征向量上操作，并将观测结果投射到希尔伯特空间中。采用随机梯度下降法(SGD)对模型进行在线训练和更新。实验表明，希尔伯特映射的精度可与高斯过程占用映射(GPOM)相比较，但它的时间复杂度与训练数据集中样本的数量呈线性关系。

Schaefer提出的离散余弦变换(DCT)映射。DCT映射为空间的每个点分配一个激光雷达衰减率，该衰减率模拟了空间对激光的局部渗透率。这样，地图就可以描述不同的激光渗透性障碍，从完全不透明到完全透明。在离散频域上表示DCT映射，并利用逆DCT的连续扩展将其转化为位置域上的连续可微场。对于相同的内存需求，DCT映射比OGM、GPOM和Hilbert映射更准确地表示LIDAR数据。尽管如此，上述连续度量表示仍然比OGM慢，尚未广泛适用于大规模实时的自动驾驶场景。

**拓扑表示路线图**的一种更复杂的表示是拓扑图，它将环境描述为一个图形模型，其中顶点表示位置，边表示它们之间的拓扑关系。拓扑图可以包含更复杂的信息，包括多车道、车道交叉口和车道合并。针对2007年DARPA城市挑战赛，提出了路线网络定义文件（RNDF），这是一个拓扑图，定义为指定无人驾驶汽车运行路段的格式化文件。根据该文件，道路网络包括一个或多个路段，每个路段包括一个或多个车道。路段的特征是车道数、街道名称和速度限制。车道的特征是车道的宽度、车道标线和一组航路点。车道之间的连接以出口和入口航路点为特征。厄姆森等人。URM08使用RNDF的图表模型作为自动驾驶汽车的老板（卡内基梅隆大学的汽车在2007年DARPA城市挑战赛中获得第一名）。图中的每个节点表示一个航路点，方向边缘表示将该节点连接到它可以到达的所有其他航路点的车道。基于多个因素的组合，将成本分配给边缘，这些因素包括穿过与边缘相关联的车道的预期时间、车道长度和环境的复杂性。Ramm提出了OpenStreetMap（OSM），它使用节点、方式和关系这三个基本体用拓扑图来建模环境。节点表示地理点，方式表示节点列表（多段线），关系由任意数量的成员组成，这些成员可以是三种类型中的任何一种，并且具有指定的角色。其他道路特性（如行驶方向和车道数）作为元素的特性给出。Bender提出了一个高度详细的拓扑路线图，称为lanelet地图，用于自动车辆泊位。lanelet地图包括道路的几何和拓扑特征，如道路、车道和交叉口，使用原子互联的可驾驶路段，称为lanelets。lanelet的几何图形由左边界和右边界定义，每个边界对应一个点列表（多段线）。此表示隐式定义每个车道的宽度和形状及其驾驶方向。lanelet的邻接构成一个加权有向图，其中每个lanelet表示一个顶点，lanelet的长度表示其出边的权重。其他元素描述了限制条件，如速度限制和交通规则，如交叉口和合并权。lanelet地图在历史悠久的Bertha-Benz纪念路线上进行了103公里的自动测试。高清地图（HD-maps）是为无人驾驶汽车提供动力的新一代拓扑地图。高清地图具有厘米级的高精度，包含丰富的信息，如车道位置、道路边界和道路曲率。由于创建高清地图的成本很高，因此有一些平台可以作为服务提供高清地图。Dharia对顶级供应商进行了评估和排名，分别是Google、HERE、TomTom和Apple。

# **路线图建图**

在地面标记和交通信号等有规则的道路和高速公路上，自动驾驶汽车需要道路地图。

从航空图像自动生成道路图的方法。

# 障碍物识别

分几步：

- 当激光雷达获取三维点云数据后，对障碍物进行一个聚类，如紫色包围框，就是识别在道路上的障碍物，它可能是动态也可能是静态的。
- 最难的部分就是把道路上面的障碍物聚类后，提取三维物体信息。获取到新物体之后，会把这个物体放到训练集里，然后用 SVM 分类器把物体识别出来。对每个物体，可能会把它的反射强度、横向和纵向的宽度以及位置姿态作为它的特征，进行提取，进而做出数据集，用于训练。最终的车辆、行人、自行车等物体的识别是由SVM分类器来完成。

# 自动驾驶中的多维感知融合技术

![img](https://img-blog.csdnimg.cn/20210905193154271.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

### 难点1 多传感器出发时间同步问题

激光雷达采样频率10hz，摄像头采样频率15hz，

![img](https://img-blog.csdnimg.cn/20210912161556479.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

###  难点2 传感器的标定算法

![img](https://img-blog.csdnimg.cn/2021100312171759.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

# 感知

### 交通信号灯的检测和识别

检测汽车周围环境中一个或多个交通信号灯的位置，并识别它们的状态(红、绿、黄)。交通信号灯的检测与识别方法主要分为两类：基于模型的和基于学习的。根据颜色和形状信息，交通灯有一个定义明确的结构。一个普通的交通灯有三个灯泡(每个状态一个：红、绿、黄)，形状很明确。因此，早期的交通灯检测识别方法大多是基于模型的。这些方法依赖于手工制作的特征工程，试图利用人类对物体颜色和形状的信息来建立一个能够检测和/或识别它的模型，当假设没有被严格遵守时，信息是不可靠的。为了增强鲁棒性，提出了不同特征(如颜色、形状和结构)的组合。例如，Zhang提出了一个多特征系统，该系统结合了颜色(使用颜色分割)、形状/结构(使用黑匣子检测)和地理信息(只使用已知红绿灯时的系统)。然而，他们的系统受到基于模型方法中常见的大量超参数的困扰，这通常意味着在某些情况下需要重新校准。作者在一个内部的私人数据集上进行了实验，并表示，失败是由于过度曝光、遮挡、非标准安装的交通灯，以及其他一些在现实世界中并不罕见的情况。在基于模型的方法的上下文中，这种组合是不够的。因此，研究者开始引入基于学习的方法。

在基于学习的方法中，特征仍然是手工制作的，但检测和/或识别过程从基于规则改为基于学习。级联分类器可能是基于学习的方法的第一次尝试。最后，还研究了HoG和Gabor特征与分类器的流行组合。最近，**端到端方法**(即，不需要手工制作的特性)的性能超过了大多数基于模型的方法。Fernandes使用GPS数据和交通灯位置数据库来识别图像中感兴趣的区域，并使用卷积神经网络(CNN)来识别交通灯状态。此外，最先进的通用目标探测器成功应用于交通信号灯的检测(通常不承认其状态)。这些通用的深度对象检测器(或简单的深度检测器，因为他们经常被称为)，全面地说，不提供关于交通灯检测和识别任务的性能的分解。尽管如此，与基于模型的方法不同的是，这些深度检测器在过度曝光、颜色失真、遮挡等情况下更健壮。Jensen对这些应用于交通灯检测的深度检测器进行了更完整的讨论。其中，作者将YOLO，在使用LISA的训练集时实现了90.49%的AUC。然而，当使用来自另一个数据集的训练数据时，性能下降到58.3%。尽管如此，它仍然是对以前方法的改进，这表明仍然有很多工作要做。基于学习的方法，特别是那些使用深度学习的方法，需要大量的注释数据。直到最近，带有注释的交通灯的大型数据库才被公开，使基于学习的方法成为可能和动力。

尽管交通灯检测和识别研究取得了进展，但人们对研究中的自动驾驶汽车使用的是什么知之甚少。也许，其中一个主要原因是在2007年的DARPA城市挑战赛中没有交通灯。本次挑战的第一和第二名认识到交通信号灯导致了城市环境的复杂性，而他们当时无法处理这些问题。2010年，Stadtpilot项目在德国布伦瑞格的公共道路上展示了他们的自动驾驶汽车Leonie。Leonie利用地图上的交通灯位置信息和C2X通信来识别交通灯的状态。然而，在演示过程中，当C2X不可用时，副驾驶不得不进入红绿灯状态。2013年，卡耐基梅隆大学的这款车使用了摄像头来检测交通灯，并使用了车辆与基础设施(V2I)通信来从配备DSRC的交通灯中获取信息。2013年，梅赛德斯-奔驰在梅赛德斯-奔驰纪念大道(上测试了他们的机器人汽车Bertha。Bertha使用视觉传感器和事先(手工)信息来检测交通灯并识别其状态。然而，对于50米以上距离的交通灯识别率需要提高。

### 车道线检测 Lane detection

主要采用基于 Camera计算视觉的算法（传统图像处理）：

- 边缘检测( Edge detection)，基于图像的灰度变化进行度量、检测和定位。其本质是计算图片在每个点的梯度（像素值的变化情况）。变化值越大表明是边缘的概率越高。边缘检测三个步骤：1对获取到的图片预处理，原始图像先通过处理变成一张灰度图，然后做图像增强；2对车道线进行特征提取，首先把经过图像增强后的图片进行二值化（ 将图像上的像素点的灰度值设置为 0 或 255，也就是将整个图像呈现出明显的黑白效果），然后做边缘提取；3直线拟合。
- 色彩阈值化，基于车道线的颜色（白色和黄色），在图像RGB色彩空间中进行过滤，以提取车道线像素。
- 基于深度学习的车道线检测算法，基于 Cameral或基于Lidar

车道线检测难点在于，对于某些车道线模糊或车道线被泥土覆盖的情况、对于黑暗环境或雨雪天气或者在光线不是特别好的情况下，它对摄像头识别和提取都会造成一定的难度。

### 信号灯、交通标志物感知

交通信号灯感知 主要采用基于camera计算视觉的算法（CNN）。下面给出 Apollo的实现思路：使用高精地图的信息+定位，为车辆提供信号灯的大致位置；利用2个 Camera(远距25mm+广角6mm)，感知识别信号灯。

### 可行驶区域感知/语义分割 semantic segmentation

基于camera或lidar的FCNN算法，实现像素级分割。

### 障碍物识别

![img](https://img-blog.csdnimg.cn/20211003123758753.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

 ![img](https://img-blog.csdnimg.cn/20211003124041113.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)



![img](https://img-blog.csdnimg.cn/20211003124003844.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

 ![img](https://img-blog.csdnimg.cn/20211003124134154.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemFzaGl6aGkzMjk5,size_20,color_FFFFFF,t_70,g_se,x_16)

# 移动物体跟踪MOT

运动目标跟踪MOT（Moving Object Tracking）子系统（也称为多目标检测与跟踪-DATMO）负责检测和跟踪自动驾驶汽车周围环境中运动障碍物的姿态。该子系统对于使自主车辆做出决策和避免与潜在移动物体（如其他车辆和行人）碰撞至关重要。随着时间的推移，移动障碍物的位置通常是根据测距传感器（如激光雷达和雷达）或立体相机捕获的数据来估计的。单目摄像机的图像能够提供丰富的视觉信息，可以用来改进运动障碍假设。针对传感器测量的不确定性，采用Bayes滤波器（如Kalman和粒子滤波器）进行状态预测。MOT方法主要分为六类：传统的、基于模型的、基于立体视觉的、基于栅格地图的、基于传感器融合的和基于深度学习的。一篇综述**Self-Driving Cars: A Survey 2019**。分成六种方法：

- 传统方法：data segmentation, data association, and filtering；
- 基于模型方法：physical models of sensors + geometric models of objects + non-parametric filters；
- 基于立体视觉方法：color + depth；
- 基于栅格图方法：occupancy grid map of the dynamic environment；
- 基于传感器融合方法：LIDAR + RADAR + camera；
- 基于深度学习方法：CNN/RNN。

**1、传统的MOT**

三个步骤：数据分割、数据关联和过滤。在数据分割步骤中，利用聚类或模式识别技术对传感器数据进行分割。在数据关联步骤中，使用数据关联技术将数据片段与目标(移动障碍)关联起来。在滤波阶段，对于每个目标，取分配给该目标的数据的几何平均值来估计其位置。位置估计通常通过卡尔曼滤波或粒子滤波来更新。Amaral等人[5]提出了一种利用3D激光雷达传感器检测和跟踪移动车辆的传统方法。利用欧氏距离对三维激光雷达点云进行点簇分割。在当前扫描传感器中观察到的障碍物(簇)与使用最近邻算法在以前扫描中观察到的障碍物相关联。利用粒子滤波算法估计障碍物的状态。速度超过给定阈值的障碍物被认为是移动的车辆。Zhang[22]为每个集群构建一个立方体包围盒，并使用盒维来区分集群是否为车辆。数据关联采用优化算法求解。采用多假设跟踪(Multiple Hypothesis Tracking, MHT)算法减少关联误差。Hwang等人[112]使用单目摄像机捕捉的图像过滤不属于移动物体(行人、自行车和车辆)的3D LIDAR点。一旦过滤，目标跟踪是基于分割匹配技术，利用从图像和3D点提取的特征。

**2、基于模型的方法**

利用传感器的物理模型和物体的几何模型直接从传感器数据推断，并使用非参数滤波器(如粒子滤波器)。不需要数据分割和关联步骤，因为几何对象模型将数据与目标关联起来。Petrovskaya提出了自动驾驶汽车Junior采用的基于模型的方法来检测和跟踪移动中的车辆。移动车辆假设检测使用不同的激光雷达数据之间的连续扫描。通过更新每个车辆目标的状态，引入新的传感器数据，而不是分离数据分割和关联步骤，包括车辆姿态和几何形状。这是通过结合卡尔曼滤波和粒子滤波(RBPF)的混合公式实现的。He等人对其修订，提出将RBPF与尺度级数粒子滤波(SSF)相结合，在整个跟踪过程中进行几何拟合和运动估计。几何形状成为一个跟踪变量，这意味着它以前的状态也可以用来预测当前状态。Vu提出一种基于模型的MOT方法，该方法旨在通过激光测量滑动时间窗口，找到最可能的移动障碍轨迹。轨迹是物体在满足测量模型和运动模型的约束条件下随时间产生的物体形状序列(L-shape、I-shape和质量点)。由于该方案计算复杂度高，他们采用了数据驱动马尔可夫链MonteCarlo (DD-MCMC)技术，可以有效地遍历解空间，找到最优解。DDMCMC的目的是在给定一组时间间隔内的观测数据的情况下，对一组航迹的概率分布进行抽样。在每次迭代中，DD-MCMC根据建议分布从当前状态采样一个新的状态(轨迹集)。新的候选状态以给定的概率被接受。为提供DD-MCMC的初步建议，从激光测量中检测出位于占用网格地图的自由或未探索区域的动态段，并通过将预定义的对象模型拟合到动态段中生成移动障碍假设。Wang采用了与基于模型的方法类似的方法，但他们没有对移动对象假设先验类别。一个贝叶斯滤波器负责联合估计传感器的姿态，静态局部背景的几何形状，以及物体的动力学和几何形状。几何信息包括二维激光雷达获得的边界点。基本上，系统通过迭代地更新跟踪状态并将新度量与当前目标关联起来进行操作。分层数据关联工作在两个级别。在第一级，新的观测值(即点的聚类)与静态目标的当前动态相匹配。在第2个关卡中，对障碍物的边界点进行更新。

**3、基于立体视觉的方法**

依靠立体图像对提供的颜色和深度信息来检测和跟踪环境中的移动障碍物。Ess提出了一种仅使用前视立体摄像机同步视频进行障碍物检测和识别的方法。他们的工作重点是基于行人和汽车探测器每帧输出的障碍跟踪。在障碍物检测方面，他们使用了一个具有定向梯度直方图HOG特征的支持向量机SVM分类器，将每个图像区域分类为障碍物或非障碍物。对于障碍物跟踪，他们采用一种假设-验证策略来拟合一组轨迹到潜在检测到的障碍物，这样这些轨迹一起具有较高的后验概率。候选轨迹集由扩展卡尔曼滤波器(EKFs)生成，初始化障碍物检测。最后，使用了一种模型选择技术，仅保留了一组最小且无冲突的轨迹，这些轨迹可以解释过去和现在的观测结果。Ziegler对于MOT，采用半全局匹配(Semi-Global Matching, SGM)方法从立体图像对中重建密集视差图像。3D环境中的所有障碍都是由一组细且垂直方向的矩形(称为超像素或Stixels)来近似表示的。利用卡尔曼滤波器跟踪Stixels的时间。最后，利用空间、形状和运动约束将Stixels分割为静态背景和移动障碍。该时空分析由基于外观的检测和识别方案补充，该方案利用类别特定的(行人和车辆)模型，增加了视觉感知的鲁棒性。实时识别主要包括三个阶段：感兴趣区域的生成、障碍物分类和目标跟踪。Chen使用半全局匹配算法从立体图像对计算视差图。在简单线性迭代聚类算法生成的图像分割中，利用视差图将边界分为共面边界、铰链边界和遮挡边界。采用改进的随机样本一致性算法进行自运动估计，得到运动点。最后，根据边界类型及其移动情况，通过合并超像素提取移动障碍。

**4、基于网格地图的方法**

首先构建动态环境的占用网格地图(Petrovskaya)。地图构建步骤之后是数据分割、数据关联和过滤步骤，以提供场景的对象级表示。Nguyen提出了一种基于网格的利用立体相机检测和跟踪运动目标的方法。他们的工作重点是行人检测和跟踪。从立体图像对重建三维点。在此基础上，利用反传感器模型估计网格地图中各单元的占用概率。采用分层分割方法，根据网格单元间的区域距离对网格单元进行聚类。最后，采用交互式多模型IMM方法对移动障碍物进行跟踪。Azim使用八轴树的3D局部占用网格地图，将环境划分为已占用、自由和未知体素。构建局部网格地图后，可以根据局部网格地图中观测到的自由空间与已占用空间之间的不一致来检测移动障碍。动态体素被聚类成运动对象，这些运动对象又被进一步划分为层。使用从每一层提取的几何特征，移动的物体被分类为已知的类别(行人、自行车、汽车或公共汽车)。Ge利用2.5D占用网格地图来模拟静态背景并检测移动障碍物。网格单元存储二维投影落入网格空间域的三维点的平均高度。从当前网格和背景模型之间的差异中检测运动假设。

**5、基于传感器融合的MOT**

基于传感器融合的方法融合来自各种传感器。Darms提出采用的基于传感器融合的方法来检测和跟踪移动车辆。MOT子系统分为两层。传感器层从传感器数据中提取特征，这些特征可用于根据点模型或框模型描述移动障碍假设。传感器层还试图将特征与目前来自核聚变层的预测假设联系起来。不能与现有假设相关联的特征被用来产生新的提议。对与给定假设相关的每个特征生成一个观察，并封装更新假设状态估计所需的所有信息。融合层根据传感器层提出的建议和观测结果，对每个假设选择最佳的跟踪模型，并使用卡尔曼滤波对假设状态进行估计(或更新估计)。Darm提出的之前的MOT子系统被扩展为利用摄像机数据，以识别移动物体的类别(如汽车、行人和自行车)，并增强来自汽车级主动传感器(如激光雷达和雷达)的测量。Mertz使用的扫描线可以直接从2D激光雷达、3D激光雷达在2D平面上的投影或从多个传感器(激光雷达、雷达和相机)的融合获得。扫描线转换为世界坐标并分割。提取每段的线和角特征。段与现有的障碍和目标的运动学更新使用卡尔曼滤波器。Na将多个传感器(如雷达、2D激光雷达和3D激光雷达)产生的移动障碍物的轨迹合并。二维激光雷达数据投影到二维平面上，使用联合概率数据关联滤波器(JPDAF)跟踪移动障碍物。将三维激光雷达数据投影到图像上，利用区域增长算法将其分割为移动障碍物。最后，利用迭代最近点(ICP)匹配或基于图像的数据关联估计或更新轨迹位姿。Xu在给定行为背景下，道路网络产生ROI。在ROI内找到候选目标并投影到道路坐标中。保持距离目标是通过将来自不同传感器(激光雷达、雷达和相机)的所有候选目标关联起来得到的。Xue融合激光雷达和摄像机数据来提高行人检测的准确性。他们利用行人高度的先验知识来减少误检。他们根据针孔相机方程(结合相机和激光雷达测量)来估计行人的高度。

**6、基于深度学习的MOT**

使用深度神经网络来检测移动障碍物的位置和几何形状，并基于当前摄像机数据跟踪它们的未来状态。Huval提出使用Overfeat卷积神经网络(CNN) 和单目输入图像，重点关注实时性。Overfeat CNN的目标是，仅通过汽车的后视镜，就能预测在同一驾驶方向上的汽车的位置和距离(深度)。Mutz跟踪方法建立在使用回归网络的通用目标跟踪(GOTURN) 之上。GOTURN是一种预先训练的深度神经网络，无需进一步训练或对象特定的微调，就能跟踪一般对象。最初，GOTURN接收到一幅图像和一个手动划分的前导车辆边界框作为输入。假设感兴趣的对象在边界框的中心。随后，对于每一幅新图像，GOTURN都会给出边界框的位置和几何形状(高度和宽度)的估计作为输出。领头车辆的位置是用激光雷达点估计的，这些点落在边界框内，并被认为是车辆。

而应用深度学习在目标跟踪中的方法可总结为四种途径：

1) **特征学习（表观模型部分）.** 如经典的CNN

2) **数据相关部分.** 比如Siamese networks 加 Hungarian/LSTM

3) **端到端学习法（比较具有挑战性）.** 如RNN/LSTM

4) **状态预测（运动/轨迹）.** 如Behavior-CNN，Social-LSTM，Occupancy Grid Map等等

# 自动驾驶感知研究方向

环境感知：

1. 目标检测和追踪（根据激光雷达点云数据估计目标的位置、大小和方向）
2. 语义分割（给图像中每个像素分配一个类别）
3. Flow（估计图像中的像素和点云中的每个点下一时刻的运动）
4. 深度估计（判断图像中每个像素的深度）
5. 行人位置估计（估计行人运动，主要是行人关节运动估计）
6. 高精度地图（根据各种传感器输入建立高精度地图）

感知中Scalability的五个方向

1. 模型泛化能力（模型在不同天气、城市和长尾问题的泛化能力）
2. Quality（模型的检测性能）
3. 模型的计算效率（内存和计算速度）
4. 自动标注（替代人工标注）
5. 仿真数据生成或数据压缩

# 自动驾驶可解释AI综述

《Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions》

在目前的技术水平下，自动驾驶汽车中的智能决策通常不为人类所理解，这种缺陷阻碍了这项技术被社会接受。因此，除了做出安全的实时决策外，自动驾驶汽车的AI系统还需要解释这些决策是如何构建的，以便在多个政府管辖区内符合监管要求。

# 自动驾驶数据

自动驾驶研发需要搜集大量数据，特别是采用深度学习训练模型。

![img](https://img-blog.csdnimg.cn/img_convert/49783d0ff1b81e7f99e1217330f76b03.png)

**1 Berkeley deep drive BDD 100k** [http://bdd-data.berkeley.edu](https://link.zhihu.com/?target=http%3A//bdd-data.berkeley.edu/)[/](https://link.zhihu.com/?target=http%3A//bdd-data.berkeley.edu/).

**2 Baidu Apolloscope Dataset [http://apolloscape.auto](https://link.zhihu.com/?target=http%3A//apolloscape.auto/)****[/](https://link.zhihu.com/?target=http%3A//apolloscape.auto/)**

**3** **Comma.ai Dataset** [https://](https://link.zhihu.com/?target=https%3A//archive.org/details/comma-dataset)[archive.org/details/comma-dataset](https://link.zhihu.com/?target=https%3A//archive.org/details/comma-dataset).

**4** **Oxford’s robotic car Dataset** [http://robotcar-dataset.robots.ox.ac.uk](https://link.zhihu.com/?target=http%3A//robotcar-dataset.robots.ox.ac.uk/)[/](https://link.zhihu.com/?target=http%3A//robotcar-dataset.robots.ox.ac.uk/).

**5** **CityScapesDataSet** [https://www.cityscapes-dataset.com](https://link.zhihu.com/?target=https%3A//www.cityscapes-dataset.com/)[/](https://link.zhihu.com/?target=https%3A//www.cityscapes-dataset.com/)

**6** **KITTI Dataset** [The KITTI Vision Benchmark Suite](https://link.zhihu.com/?target=http%3A//www.cvlibs.net/datasets/kitti/raw_data.php)

**7 Ford Campus Vision and Lidar dataset** [http://](https://link.zhihu.com/?target=http%3A//robots.engin.umich.edu/SoftwareData/Ford)[robots.engin.umich.edu/SoftwareData/Ford](https://link.zhihu.com/?target=http%3A//robots.engin.umich.edu/SoftwareData/Ford).

**8 CamVid** http://mi.eng.cam.ac.uk/research/projects/VideoRec/CamVid

**9 TuSimple Dataset** [http://benchmark.tusimple.ai/#/](https://link.zhihu.com/?target=http%3A//benchmark.tusimple.ai/)

**10** **CMU Visual Localization Dataset** [http://3dvis.ri.cmu.edu/data-sets/localization](https://link.zhihu.com/?target=http%3A//3dvis.ri.cmu.edu/data-sets/localization/)[/](https://link.zhihu.com/?target=http%3A//3dvis.ri.cmu.edu/data-sets/localization/).

**11 NuScenes (Nutonomy-Aptiv dataset)** [https://www.nuscenes.org/](https://link.zhihu.com/?target=https%3A//www.nuscenes.org/)

**12 KUL Belgium Traffic Sign Dataset** [http://www.vision.ee.ethz.ch/~timofter/traffic_signs/](https://link.zhihu.com/?target=http%3A//www.vision.ee.ethz.ch/~timofter/traffic_signs/)

**13 MIT AGE Lab Dataset** [http://lexfridman.com/carsync](https://link.zhihu.com/?target=http%3A//lexfridman.com/carsync).

**14** **LISA: Laboratory for Intelligent & Safe Automobiles, UC San Diego Datasets**

[Laboratory for Intelligent and Safe Automobiles](https://link.zhihu.com/?target=http%3A//cvrr.ucsd.edu/LISA/datasets.html)

**15 Udacity challenge Datasets**

[https](https://link.zhihu.com/?target=https%3A//www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013)[://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013](https://link.zhihu.com/?target=https%3A//www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013).

**16** **VelodyneSLAM Dataset** [http://www.mrt.kit.edu/z/publ/down](https://link.zhihu.com/?target=http%3A//www.mrt.kit.edu/z/publ/download/velodyneslam/dataset.html)

**17 The UAH-DriveSet** [http://www.robesafe.uah.es/persona](https://link.zhihu.com/?target=http%3A//www.robesafe.uah.es/personal/eduardo.romera/uah-driveset/)

**标注数据**是劳动密集行业，标注工具需要开发，也有些开源工具可以利用来二次开发，有不少公司提供这种业务。

其中激光雷达数据注释成本非常高。精确的激光雷达传感器模型可以应对这种问题：《End-to-end sensor modeling for LiDAR Point Cloud》模拟生成激光点云数据。提出了一种基于深度学习的激光雷达传感器模型。该方法使用深度神经网络模拟传感器回波信号，用极坐标网格图（Polar Grid Maps，PGM）对从实际数据中学习的回波脉冲宽度进行建模。

# 自动驾驶仿真

与自动驾驶仿真相关的软件。

1. 专门的自动驾驶模拟仿真软件，如Prescan、VTD、51sim-one、Panosim、GaiA等等。
2. 基于游戏引擎做的自动驾驶仿真软件，主要代表是基于Unity的Lgsvl Simulator、baidu-Unity，基于Unreal的Carla、Airsim等。
3. 基于一些机器人仿真软件做的自动驾驶仿真器，如基于ROS的Gazebo、rviz开发的仿真平台，基于blender开发的平台等等。

# 协同驾驶

参考文献：

[1][ 如何看待本田的L3自动驾驶将在日本上路？](https://mp.weixin.qq.com/s/PxnpKHTvvIafN4kZGpZh_g) 如何看待本田的L3自动驾驶将在日本上路？

[2] 长篇自动驾驶技术综述论文（上） - 知乎  https://zhuanlan.zhihu.com/p/142441973

[3] 特斯拉公开为什么干掉雷达！AI主管亲自演讲解读 https://m.weibo.cn/status/4652363509668140?wm=3333_2001&from=10B6293010&sourcetype=weixin

[4] 自动驾驶汽车涉及哪些技术？ - 知乎  https://www.zhihu.com/question/24506695

[5] 自动驾驶中的多维感知融合技术分享（Camera+Liar+Radar+IMU）_哔哩哔哩_bilibili  https://www.bilibili.com/video/BV1A64y1k73H/?spm_id_from=333.788.recommend_more_video.0

[6] SLAM算法_在路上@Amos-CSDN博客_slam算法  https://blog.csdn.net/qq_33835307/article/details/81842998

[7] 自动驾驶中常用传感器硬件介绍_哔哩哔哩_bilibili  https://www.bilibili.com/video/BV1pU4y1G7Vp

[8] 几个主流的自动驾驶平台_Jason_Lee155的博客-CSDN博客_自动驾驶平台  https://blog.csdn.net/Jason_Lee155/article/details/119709721?spm=1001.2101.3001.6661.1&utm_medium=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link&depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-1.no_search_link

[9] 1.1 毫米波传感介绍：FMCW雷达 - 模块1：范围估计-mmWave系列培训-TI教室-EEWORLD大学堂  http://training.eeworld.com.cn/TI/video/10162

[10] 多传感器信息融合(标定, 数据融合, 任务融合)_ChenGuiGan的博客-CSDN博客_多传感器信息融合  https://blog.csdn.net/ChenGuiGan/article/details/104960658?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2.no_search_link

[11] 自动驾驶系统的传感器标定方法 - 知乎  https://zhuanlan.zhihu.com/p/57028341

[12] Waymo研发经理：《自动驾驶感知前沿技术介绍》_10点43的博客-CSDN博客  https://blog.csdn.net/cg129054036/article/details/120070621

13、谷歌：无人车重磅报告——《通往完全自动驾驶之路》.pdf  https://max.book118.com/html/2018/0421/162378877.shtm

14、自动驾驶发展面临的恶劣天气问题 - 知乎  https://zhuanlan.zhihu.com/p/447052150

15、论文《Self-Driving Cars：A Survey》翻译（2019）综述（二） - 知乎  https://zhuanlan.zhihu.com/p/419614760

16、目前传感器，感知，地图定位和规划控制的水平怎么样，他们的重要性怎么看？ - 知乎  https://zhuanlan.zhihu.com/p/55581334

17、自动驾驶研发的开源数据 - 知乎  https://zhuanlan.zhihu.com/p/56451461

18、自动驾驶中的目标跟踪方法 - 知乎  https://zhuanlan.zhihu.com/p/59890550

19、arXiv论文2019《A Survey of Deep Learning Techniques for Autonomous Driving》：自动驾驶中深度学习-综述 - 知乎  https://zhuanlan.zhihu.com/p/87416772

20、自动驾驶仿真工具需求 - 知乎  https://zhuanlan.zhihu.com/p/85208980

21、OpenCDA：一个开源的多车协同自动驾驶仿真平台 - 知乎  https://zhuanlan.zhihu.com/p/399299136

22、介绍几篇V2V和自动驾驶结合的编队协同文章 - 知乎  https://zhuanlan.zhihu.com/p/406951048

23、车辆和车路协同的具体技术 - 知乎  https://zhuanlan.zhihu.com/p/99993744

24、介绍一篇路端传感器的cooperative perception（3D目标检测）论文 - 知乎  https://zhuanlan.zhihu.com/p/287130155